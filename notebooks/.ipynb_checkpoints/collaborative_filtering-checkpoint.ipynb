{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Subreddit Recommender System Using Neural Collaborative Filtering\n",
    "\n",
    "Sources: https://arxiv.org/pdf/1708.05031.pdf, https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Not too long ago, I thought of the idea of creating a subreddit recommender system as my next project. Reddit has over 130,000 active subreddits and there's not a recommendation system in place that points you to the subreddits you might be interested in. At the time, I was also finishing Andrew Ng's deep learning specialization. I thought to myself what better way to practice what I've learned by implementing my own recommender system using a neural network. \n",
    "\n",
    "I found that there are 2 main approaches to building a recommender system: content-based filtering and collaborative filtering. Content-based filtering approaches recommend you items based on the descriptions of the items. This approach would not work well in our case since subreddits don't have specific categories or metadata to them. It would be a difficult task to figure out which subreddits have similar content. Collaborative filtering systems, on the other hand, recommend items based on other users' preferences. This is much more feasible for my application. I could collect data on a lot of users and their subreddit activities. Then, if I want to make recommendations for, let's say, the /r/python subreddit, I could see which users have been active in /r/python, what other subreddits they comment in, and recommend those subreddits to users visiting /r/python. Of course, it'll be more complicated than that since we'll be using a neural network to learn which subreddits to recommend based on user preferences.\n",
    "\n",
    "I started researching on deep learning being applied to collaborative recommender systems and found this paper done by the National University of Singapore in 2017 titled \"Neural Collaborative Filtering\". The paper proposes one of the first neural network architectures modelled for collaborative filtering and claims to outperform many of the popular collaborative filtering methods at the time. In this project, I create my own simpler implementation of the model and apply it to a user-subreddit interaction dataset that I've created. Then, I deploy the model to a web app that recommends subreddits given a subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import keras\n",
    "import multiprocessing\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "import sqlite3\n",
    "from time import time\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Embedding, Input, Dense, Multiply, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network architecture\n",
    "\n",
    "<img src=\"../media/figure3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMF(X_user, X_item, num_users, num_items, latent_dim):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implementation of the Generalized Matrix Factorization\n",
    "    \n",
    "    Arguments:\n",
    "    X_user -- input tensor of shape (1,), specifying the user id\n",
    "    X_item -- input tensor of shape (1,), specifying the item id\n",
    "    num_users -- integer, number of total users \n",
    "    num_items -- integer, number of total items\n",
    "    latent_dim -- dimension of the embedding latent vector\n",
    "    initializer -- initializer for the embeddings matrix\n",
    "    regs -- float list of size 2, specifying the regularization parameters for both embedding layers\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the GMF, tensor of shape (1,), specifies the likelihood that X_item is relevant to X_user\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer = initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=1)\n",
    "    \n",
    "    X_user = Embedding(input_dim=num_users, output_dim=latent_dim, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='GMF_user_embedding')(X_user)\n",
    "    X_item = Embedding(input_dim=num_items, output_dim=latent_dim, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='GMF_item_embedding')(X_item)\n",
    "    \n",
    "    X_user = Flatten()(X_user)\n",
    "    X_item = Flatten()(X_item)\n",
    "    \n",
    "    X = Multiply()([X_user, X_item])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_GMF_model(num_users, num_items, latent_dim, initializer='uniform'):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = GMF(X_user, X_item, num_users, num_items, latent_dim, initializer)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Layer Perceptron (MLP)\n",
    "\n",
    "The GMF only uses a fixed element-wise product between the two latent vectors to model their interactions. More flexibility and non-linearity can be obtained by also concatenating the two latent vectors and feeding the concatenation to a standard Multi-Layer Perceptron that can learn the interaction between the user and the item.\n",
    "\n",
    "The MLP model is defined as:\n",
    "\n",
    "$$\\textbf{z}_1=\\phi_1(\\textbf{p}_u,\\textbf{q}_i)=[\\textbf{p}_u\\;\\textbf{q}_i]^T$$\n",
    "\n",
    "$$\\phi_2(\\textbf{z}_1)=a_2(\\textbf(W)_2^T\\textbf{z}_1+\\textbf{b}_2)$$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$\\phi_L(\\textbf{z}_{L-1})=a_L(\\textbf{W}_L^T\\textbf{z}_{L-1}+\\textbf{b}_L)$$\n",
    "\n",
    "$$\\hat{y}_{ui}=\\sigma(\\textbf{h}^T\\phi_L(\\textbf{z}_{L-1}))$$\n",
    "\n",
    "where $\\textbf{W}_x$, $\\textbf{b}_x$, and $a_x$ denote the weight matrix, bias vector, and activation function for the x-th layer's perceptron, respectively. Various activation functions can be chosen for the MLP layers, but the paper opts to use the ReLU, because it is more biologically plausible, proven to be non-saturated, and encourages sparse activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X_user, X_item, num_users, num_items, layers = [20, 10]):\n",
    "\n",
    "    \"\"\"\n",
    "    Implementation of the Multi-Layer Perceptron\n",
    "    \n",
    "    Arguments:\n",
    "    X_user -- input tensor of shape (1,), specifying the user id\n",
    "    X_item -- input tensor of shape (1,), specifying the item id\n",
    "    num_users -- integer, number of total users \n",
    "    num_items -- integer, number of total items\n",
    "    initializer -- initializer for the embeddings matrix\n",
    "    layers -- integer list, specifying the units for each layer\n",
    "    regs -- float list, specifying the regularization parameters for each layer, reg_layers[0] is for the embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the MLP, tensor of shape (1,), specifies the likelihood that X_item is relevant to X_user\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer = initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=1)\n",
    "    \n",
    "    X_user = Embedding(input_dim=num_users, output_dim=layers[0]//2, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='MLP_user_embedding')(X_user)\n",
    "    X_item = Embedding(input_dim=num_items, output_dim=layers[0]//2, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='MLP_item_embedding')(X_item)\n",
    "    \n",
    "    X_user = Flatten()(X_user)\n",
    "    X_item = Flatten()(X_item)\n",
    "    \n",
    "    X = Concatenate()([X_user, X_item])\n",
    "\n",
    "    for i in range(1, len(layers)):\n",
    "        X = Dense(layers[i], activation='relu', name='layer'+str(i))(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_MLP_model(num_users, num_items, layers=[20,10]):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = MLP(X_user, X_item, num_users, num_items, initializer, layers)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Fusion of GMF and MLP\n",
    "\n",
    "The one-hot encoding user and item vectors can be fed to two different embeddings, one for the GMF and one for the MLP. Then, the two models can be combined by concatenating their last hidden layer. The fused model can be pictured below.\n",
    "\n",
    "<img src=\"../media/figure3.png\">\n",
    "\n",
    "The final fused model can be formulated as such:\n",
    "\n",
    "$$\\phi^{GMF}=\\textbf{p}_u^G\\odot\\textbf{q}_i^G$$\n",
    "\n",
    "$$\\phi^{MLP}=a_L(\\textbf{W}_L^T(a_{L-1}(...a_2(\\textbf{W}_2^T[\\textbf{p}_u^M\\;\\textbf{q}_i^M]^T+\\textbf{b}_2)...))+\\textbf{b}_L)$$\n",
    "\n",
    "$$\\hat{y}_{ui}=\\sigma(\\textbf{h}^T[\\phi^{GMF}\\;\\phi^{MLP}]^T)$$\n",
    "\n",
    "where $\\textbf{p}_u^G$ and $\\textbf{p}_u^M$ denote the user embedding for GMF and MLP parts, and similar notations of $\\textbf{q}_i^G$ and $\\textbf{q}_i^M$ for item embeddings. This model is dubbed as \"NeuMF\", short for Neural Matrix Factorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuMF(X_user, X_item, num_users, num_items, gmf_latent_dim=10, layers=[20, 10]):\n",
    "    X_GMF = GMF(X_user, X_item, num_users, num_items, gmf_latent_dim)\n",
    "    X_MLP = MLP(X_user, X_item, num_users, num_items, layers)\n",
    "    \n",
    "    X = Concatenate()([X_GMF, X_MLP])\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_NeuMF_model(num_users, num_items, gmf_latent_dim=10, layers=[20, 10]):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = NeuMF(X_user, X_item, num_users, num_items, gmf_latent_dim, layers)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Pre-training\n",
    "\n",
    "For training, one seeks to minimize the object function of NeuMF. However, due to the function's non-convexity, one can only find local solutions using gradient-based optimization. Initialization plays an important role for the convergence and performance of deep learning models. The paper proposes to first train GMF and MLP, first. Then, their model parameters are used as the initialization for the corresponding parts of NeuMF's parameters. In the output layer, the weights of the 2 models are concatenated:\n",
    "\n",
    "$$\\textbf{h}=[\\alpha\\textbf{h}^{GMF}\\;(1-\\alpha)\\textbf{h}^{MLP}]^T$$\n",
    "\n",
    "where $\\textbf{h}^{GMF}$ and $\\textbf{h}^{MLP}$ denote the $\\textbf{h}$ vector of the pretrained GMF and MLP model, respectively, and $\\alpha$ is a hyper-parameter determining the trade-off between the two pre-trained models.\n",
    "\n",
    "For training GMF and MLP from scratch, Adaptive Moment Estimation (Adam) is used. After feeding the pre-trained parameters into NeuMF, the ensemble model is optimized with Vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "num_negatives = 4\n",
    "batch_size = 256\n",
    "layers = [64,32,16,8]\n",
    "topK = 10\n",
    "gmf_latent_dim=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/reddit_train_10.csv', header=None)\n",
    "df_test_positive = pd.read_csv('../data/reddit_test_positive_10.csv', header=None, usecols=[0,1])\n",
    "df_test_negative = pd.read_csv('../data/reddit_test_negative_10.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = max(df_train.iloc[:, 0])\n",
    "num_items = max(df_train.iloc[:, 1])\n",
    "\n",
    "train = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "for i, row in df_train.iterrows():\n",
    "    user = row[0]\n",
    "    item = row[1]\n",
    "    train[user, item] = 1.0\n",
    "    \n",
    "test_positive = [(row[0], row[1]) for _, row in df_test_positive.iterrows()]\n",
    "test_negative = [row[1:100].values.flatten().tolist() for _, row in df_test_negative.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_NeuMF_model(num_users+1, num_items+1, layers=layers, gmf_latent_dim=gmf_latent_dim)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building Dataset\n",
    "\n",
    "### Evaluation Protocols\n",
    "\n",
    "The paper uses leave-one-out method to evaluate the performance of item recommendation algorithms. Basically, for each user, the last interaction is held-out. All held-out interactions are used as the test set, and the rest are used for training. Then, for each user, 100 items are randomly sampled, and then ranked. These are items that the user hasn't interacted with before. This method separates our dataset into three files: train.rating, test.rating, test.negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, pos_item):\n",
    "    for item in ranklist:\n",
    "        if item == pos_item:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_model(model, test_positive, test_negative, K):\n",
    "    hits = []\n",
    "    for i in range(len(test_positive)):\n",
    "        \n",
    "        rating = test_positive[i]\n",
    "        items = test_negative[i]\n",
    "        user, pos_item = rating\n",
    "        items.append(pos_item)\n",
    "        \n",
    "        map_item_score = {}\n",
    "        \n",
    "        users = np.full(len(items), user, dtype='int32')\n",
    "        predictions = model.predict([users, np.array(items)], batch_size=100, verbose=0)\n",
    "        for i in range(len(items)):\n",
    "            map_item_score[items[i]] = predictions[i]\n",
    "        items.pop()\n",
    "        \n",
    "        ranklist = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "        hr = getHitRatio(ranklist, pos_item)\n",
    "        hits.append(hr)\n",
    "        \n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.0957\t [18.2 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1086s 153us/step - loss: 0.2132\n",
      "Iteration 0 [1101.9 s]: HR = 0.8952, loss = 0.2132 [17.8 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1106s 155us/step - loss: 0.1873\n",
      "Iteration 1 [1118.3 s]: HR = 0.9114, loss = 0.1873 [20.0 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1147s 161us/step - loss: 0.1763\n",
      "Iteration 2 [1161.2 s]: HR = 0.9205, loss = 0.1763 [22.6 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1136s 160us/step - loss: 0.1706\n",
      "Iteration 3 [1150.8 s]: HR = 0.9205, loss = 0.1706 [18.2 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1099s 154us/step - loss: 0.1662\n",
      "Iteration 4 [1110.7 s]: HR = 0.9223, loss = 0.1662 [18.7 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1112s 156us/step - loss: 0.1632\n",
      "Iteration 5 [1124.3 s]: HR = 0.9224, loss = 0.1632 [19.8 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1115s 157us/step - loss: 0.1601\n",
      "Iteration 6 [1128.0 s]: HR = 0.9237, loss = 0.1601 [18.9 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1131s 159us/step - loss: 0.1576\n",
      "Iteration 7 [1145.0 s]: HR = 0.9233, loss = 0.1576 [17.5 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1110s 156us/step - loss: 0.1552\n",
      "Iteration 8 [1122.3 s]: HR = 0.9227, loss = 0.1552 [18.2 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1096s 154us/step - loss: 0.1530\n",
      "Iteration 9 [1108.2 s]: HR = 0.9231, loss = 0.1530 [19.5 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1169s 164us/step - loss: 0.1511\n",
      "Iteration 10 [1182.7 s]: HR = 0.9239, loss = 0.1511 [18.6 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1121s 158us/step - loss: 0.1492\n",
      "Iteration 11 [1133.8 s]: HR = 0.9262, loss = 0.1492 [19.4 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1102s 155us/step - loss: 0.1475\n",
      "Iteration 12 [1113.8 s]: HR = 0.9210, loss = 0.1475 [18.4 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1067s 150us/step - loss: 0.1460\n",
      "Iteration 13 [1078.6 s]: HR = 0.9240, loss = 0.1460 [20.1 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1073s 151us/step - loss: 0.1445\n",
      "Iteration 14 [1084.6 s]: HR = 0.9258, loss = 0.1445 [18.0 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1123s 158us/step - loss: 0.1431\n",
      "Iteration 15 [1134.5 s]: HR = 0.9263, loss = 0.1431 [17.9 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1092s 153us/step - loss: 0.1418\n",
      "Iteration 16 [1103.8 s]: HR = 0.9190, loss = 0.1418 [17.5 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1119s 157us/step - loss: 0.1407\n",
      "Iteration 17 [1131.3 s]: HR = 0.9255, loss = 0.1407 [17.8 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 22358s 3ms/step - loss: 0.1398\n",
      "Iteration 18 [22369.6 s]: HR = 0.9243, loss = 0.1398 [16.9 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 986s 139us/step - loss: 0.1388\n",
      "Iteration 19 [996.0 s]: HR = 0.9257, loss = 0.1388 [17.2 s]\n",
      "End. Best Iteration 15:  HR = 0.9263. \n"
     ]
    }
   ],
   "source": [
    "model_out_file = '../pretrain/reddit_NeuMF_%d_%s_%d.h5' %(gmf_latent_dim, layers, time())\n",
    "\n",
    "t1 = time()\n",
    "hits = evaluate_model(model, test_positive, test_negative, topK)\n",
    "hr = np.array(hits).mean()\n",
    "print('Init: HR = %.4f\\t [%.1f s]' % (hr, time()-t1))\n",
    "\n",
    "best_hr, best_iter = hr, -1\n",
    "for epoch in range(epochs):\n",
    "    t1=time()\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels), batch_size=batch_size, \\\n",
    "                     epochs=1, shuffle=True)\n",
    "    \n",
    "    t2 = time()\n",
    "    hits = evaluate_model(model, test_positive, test_negative, topK)\n",
    "    hr, loss = np.array(hits).mean(), hist.history['loss'][0]\n",
    "    print('Iteration %d [%.1f s]: HR = %.4f, loss = %.4f [%.1f s]' \n",
    "        % (epoch,  t2-t1, hr, loss, time()-t2))\n",
    "    if hr > best_hr:\n",
    "        best_hr, best_iter = hr, epoch\n",
    "        model.save_weights(model_out_file, overwrite=True)\n",
    "        \n",
    "print(\"End. Best Iteration %d:  HR = %.4f. \" %(best_iter, best_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_file = '../pretrain/reddit_NeuMF_%d_%s_%d.h5' %(gmf_latent_dim, layers, time())\n",
    "model.save_weights(model_out_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../pretrain/reddit_NeuMF_8_[64, 32, 16, 8]_1563000005.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_item_embedding = model.get_layer('GMF_item_embedding')\n",
    "MLP_item_embedding = model.get_layer('MLP_item_embedding')\n",
    "\n",
    "GMF_item_weights = GMF_item_embedding.get_weights()[0]\n",
    "MLP_item_weights = MLP_item_embedding.get_weights()[0]\n",
    "\n",
    "NeuMF_item_weights = np.concatenate((GMF_item_weights, MLP_item_weights),axis=1)\n",
    "NeuMF_item_weights_norma = NeuMF_item_weights/np.linalg.norm(NeuMF_item_weights, axis = 1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_weights_1,mlp_weights_2,mlp_weights_3,mlp_weights_4,mlp_weights_5,mlp_weights_6,mlp_weights_7,mlp_weights_8,mlp_weights_9,mlp_weights_10,mlp_weights_11,mlp_weights_12,mlp_weights_13,mlp_weights_14,mlp_weights_15,mlp_weights_16,mlp_weights_17,mlp_weights_18,mlp_weights_19,mlp_weights_20,mlp_weights_21,mlp_weights_22,mlp_weights_23,mlp_weights_24,mlp_weights_25,mlp_weights_26,mlp_weights_27,mlp_weights_28,mlp_weights_29,mlp_weights_30,mlp_weights_31,mlp_weights_32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x265000328f0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('../app/ncf.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "gmf_weights_columns = ','.join([ f'gmf_weights_{i+1}' for i in range(gmf_latent_dim)])\n",
    "mlp_weights_columns = ','.join([ f'mlp_weights_{i+1}' for i in range(layers[0]//2)])\n",
    "\n",
    "c.execute(f'CREATE TABLE weights (item_id, {gmf_weights_columns}, {mlp_weights_columns})')\n",
    "#c.execute(\"\")\n",
    "\n",
    "#conn.commit()\n",
    "\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32897615,  0.10641152, -0.42197388, ..., -0.04275219,\n",
       "         0.12431271,  0.02298403],\n",
       "       [ 0.3290243 ,  0.10623664, -0.37675765, ..., -0.12078571,\n",
       "         0.19027624, -0.06052374],\n",
       "       [ 0.37128088,  0.11445459, -0.39325753, ...,  0.01715298,\n",
       "         0.11689451,  0.02376154],\n",
       "       ...,\n",
       "       [ 0.2675498 ,  0.08609024, -0.3365847 , ..., -0.01009374,\n",
       "         0.19148614, -0.05352863],\n",
       "       [ 0.14706582,  0.07890623, -0.31575355, ...,  0.06139897,\n",
       "         0.22040662, -0.02438055],\n",
       "       [-0.2887616 , -0.04176541,  0.25244355, ..., -0.22265546,\n",
       "         0.08488164, -0.15461084]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuMF_item_weights_norma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/subreddit10.json') as f:\n",
    "    [d, inv_d] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit: twice           Similarity: 1.0\n",
      "Subreddit: blackpink       Similarity: 0.944\n",
      "Subreddit: twicemedia      Similarity: 0.922\n",
      "Subreddit: digimonlinkz    Similarity: 0.905\n",
      "Subreddit: japanesemusic   Similarity: 0.894\n",
      "Subreddit: supersentai     Similarity: 0.89\n",
      "Subreddit: falcom          Similarity: 0.884\n",
      "Subreddit: pocophones      Similarity: 0.883\n",
      "Subreddit: animepiracy     Similarity: 0.881\n",
      "Subreddit: magiarecord     Similarity: 0.88\n",
      "Subreddit: osuskins        Similarity: 0.878\n",
      "Subreddit: symmetramains   Similarity: 0.878\n",
      "Subreddit: bangtan         Similarity: 0.875\n",
      "Subreddit: hajimenoippo    Similarity: 0.875\n",
      "Subreddit: dissidia        Similarity: 0.874\n",
      "Subreddit: dissidiaffoo    Similarity: 0.872\n",
      "Subreddit: sololeveling    Similarity: 0.871\n",
      "Subreddit: citrusmanga     Similarity: 0.87\n",
      "Subreddit: izone           Similarity: 0.869\n",
      "Subreddit: koreanvariety   Similarity: 0.868\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(subreddit, num_recommendations):\n",
    "    index = inv_d[subreddit.lower()]\n",
    "    dists = np.dot(NeuMF_item_weights_norma, NeuMF_item_weights_norma[int(index)])\n",
    "    sorted_dists = np.argsort(dists)\n",
    "    closest = sorted_dists[-num_recommendations:]\n",
    "    max_width = max([len(d[str(c)]) for c in closest])\n",
    "    for c in reversed(closest):\n",
    "        print(f'Subreddit: {d[str(c)]:{max_width + 2}} Similarity: {dists[c]:.{3}}')\n",
    "        \n",
    "get_recommendations('twice', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('../pretrain/reddit_NeuMF_8_[64, 32, 16, 8]_1563000005.h5', 'r')\n",
    "d = f['GMF_item_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54926, 8)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['GMF_item_embedding']['GMF_item_embedding_1']['embeddings:0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54926, 32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['MLP_item_embedding']['MLP_item_embedding_1']['embeddings:0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
