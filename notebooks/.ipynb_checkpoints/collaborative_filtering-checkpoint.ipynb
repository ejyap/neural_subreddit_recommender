{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1708.05031.pdf\n",
    "\n",
    "Paper's Code: https://github.com/hexiangnan/neural_collaborative_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import keras\n",
    "import multiprocessing\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "import sqlite3\n",
    "from time import time\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Embedding, Input, Dense, Multiply, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network architecture\n",
    "\n",
    "<img src=\"../media/figure3.png\">\n",
    "\n",
    "The model takes as input two one-hot encoded vectors, one for the user and one for the subreddit. Each training example takes a binary label, 0 or 1. 0 if the user has never commented in the subreddit. 1 if the user has commented in the subreddit. \n",
    "\n",
    "Each vector passes through two embedding layers, the General Matrix Factorization embedding layer and the Multi-layer Perceptron embedding layer. These are fed to the GMF and MLP. \n",
    "\n",
    "The two output vector are then concatenated and passed through a final sigmoid-activation layer that predicts a number between 0 or 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Matrix Factorization (GMF) Layer\n",
    "\n",
    "This component simply transforms the user and item vectors into their corresponding embedding vectors. The two are then multiplied elementwise. It takes as a hyperparameter a latent_dim, which specifies the dimension of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMF(X_user, X_item, num_users, num_items, latent_dim):\n",
    "    \n",
    "    initializer = initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=1)\n",
    "    \n",
    "    X_user = Embedding(input_dim=num_users, output_dim=latent_dim, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='GMF_user_embedding')(X_user)\n",
    "    X_item = Embedding(input_dim=num_items, output_dim=latent_dim, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='GMF_item_embedding')(X_item)\n",
    "    \n",
    "    X_user = Flatten()(X_user)\n",
    "    X_item = Flatten()(X_item)\n",
    "    \n",
    "    X = Multiply()([X_user, X_item])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_GMF_model(num_users, num_items, latent_dim, initializer='uniform'):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = GMF(X_user, X_item, num_users, num_items, latent_dim, initializer)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP) Layer\n",
    "\n",
    "The MLP component also transforms the user and item vectors into embeddings. They are then concatenated and fed to a standard multi-layer perceptron. It takes as a hyeprparameter a list specifying the layer dimensions. The first layer specifies the dimensions of the embedding layer, and the rest of the layers specify the dimensions of the multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X_user, X_item, num_users, num_items, layers = [20, 10]):\n",
    "\n",
    "    initializer = initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=1)\n",
    "    \n",
    "    X_user = Embedding(input_dim=num_users, output_dim=layers[0]//2, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='MLP_user_embedding')(X_user)\n",
    "    X_item = Embedding(input_dim=num_items, output_dim=layers[0]//2, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='MLP_item_embedding')(X_item)\n",
    "    \n",
    "    X_user = Flatten()(X_user)\n",
    "    X_item = Flatten()(X_item)\n",
    "    \n",
    "    X = Concatenate()([X_user, X_item])\n",
    "\n",
    "    for i in range(1, len(layers)):\n",
    "        X = Dense(layers[i], activation='relu', name='layer'+str(i))(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_MLP_model(num_users, num_items, layers=[20,10]):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = MLP(X_user, X_item, num_users, num_items, initializer, layers)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion of GMF and MLP\n",
    "\n",
    "The outputs of the GMF and MLP are concatenated together and passed through a final layer that predicts the label using the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuMF(X_user, X_item, num_users, num_items, gmf_latent_dim=10, layers=[20, 10]):\n",
    "    X_GMF = GMF(X_user, X_item, num_users, num_items, gmf_latent_dim)\n",
    "    X_MLP = MLP(X_user, X_item, num_users, num_items, layers)\n",
    "    \n",
    "    X = Concatenate()([X_GMF, X_MLP])\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_NeuMF_model(num_users, num_items, gmf_latent_dim=10, layers=[20, 10]):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = NeuMF(X_user, X_item, num_users, num_items, gmf_latent_dim, layers)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For training the models, I used the hyperparameters suggested by the paper: latent_dim=8 and layers=[64,32,16,8]. This means that the GMF will learn an 8-dimensional embedding, while the MLP will learn a 32-dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_latent_dim=8\n",
    "layers = [64,32,16,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "After collecting the data through Reddit's API, I had to separate it into a training and test set, and format it, accordingly. The code for this process can be found [here](https://github.com/ejyap/neural_subreddit_recommender/blob/master/notebooks/perpare_datasets.ipynb).\n",
    "\n",
    "First, I deleted all duplicated user-subreddit interactions. I kept only users that had commented in 10 unique subreddits. \n",
    "\n",
    "The paper uses the leave-one out evaluation method. For each user, the last interaction was held out as the test set, and the remaining interactions were used for the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/reddit_train_10.csv', header=None)\n",
    "df_test_positive = pd.read_csv('../data/reddit_test_positive_10.csv', header=None, usecols=[0,1])\n",
    "df_test_negative = pd.read_csv('../data/reddit_test_negative_10.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = max(df_train.iloc[:, 0])\n",
    "num_items = max(df_train.iloc[:, 1])\n",
    "\n",
    "train = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "for i, row in df_train.iterrows():\n",
    "    user = row[0]\n",
    "    item = row[1]\n",
    "    train[user, item] = 1.0\n",
    "    \n",
    "test_positive = [(row[0], row[1]) for _, row in df_test_positive.iterrows()]\n",
    "test_negative = [row[1:100].values.flatten().tolist() for _, row in df_test_negative.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "The performance of the mdoel was evaluated using the Hit Ratio (HR). The hit ratio takes in a list of recommended items and a specific item. It returns 1 if the item is in the recommended list; it returns 0, otherwise.\n",
    "\n",
    "For each user, I sampled 99 non-interacted subreddits and combined it with the interacted subreddit. The model generates a ranked recommended list for the user consisting of these 100 subreddits. If the interacted subreddit is in the top 10 (topK=10), the Hit Ratio returns 1. If it's in any of the other 90 ranks, the hit ratio returns 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = 10\n",
    "\n",
    "def getHitRatio(ranklist, pos_item):\n",
    "    for item in ranklist:\n",
    "        if item == pos_item:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_model(model, test_positive, test_negative, K):\n",
    "    hits = []\n",
    "    for i in range(len(test_positive)):\n",
    "        \n",
    "        rating = test_positive[i]\n",
    "        items = test_negative[i]\n",
    "        user, pos_item = rating\n",
    "        items.append(pos_item)\n",
    "        \n",
    "        map_item_score = {}\n",
    "        \n",
    "        users = np.full(len(items), user, dtype='int32')\n",
    "        predictions = model.predict([users, np.array(items)], batch_size=100, verbose=0)\n",
    "        for i in range(len(items)):\n",
    "            map_item_score[items[i]] = predictions[i]\n",
    "        items.pop()\n",
    "        \n",
    "        ranklist = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "        hr = getHitRatio(ranklist, pos_item)\n",
    "        hits.append(hr)\n",
    "        \n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling\n",
    "\n",
    "The model is compiled with an adam optimizer and the binary cross-entropy loss.  Nevertheless, the goal is to optimize the hit ratio, not the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_NeuMF_model(num_users+1, num_items+1, layers=layers, gmf_latent_dim=gmf_latent_dim)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "In addition to the user-subreddit interactions in the training set, we also have to train the model on non-interacted subreddits. So, for each interaction, we sample 4 \"non-interactions\" (num_negatives=4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 4\n",
    "\n",
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for 20 epochs. Since the goal is to optimize the hit ratio, the model that yields the best average hit ratio is saved. We also save the last iteration, in case I want to train further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.0957\t [18.2 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1086s 153us/step - loss: 0.2132\n",
      "Iteration 0 [1101.9 s]: HR = 0.8952, loss = 0.2132 [17.8 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1106s 155us/step - loss: 0.1873\n",
      "Iteration 1 [1118.3 s]: HR = 0.9114, loss = 0.1873 [20.0 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1147s 161us/step - loss: 0.1763\n",
      "Iteration 2 [1161.2 s]: HR = 0.9205, loss = 0.1763 [22.6 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1136s 160us/step - loss: 0.1706\n",
      "Iteration 3 [1150.8 s]: HR = 0.9205, loss = 0.1706 [18.2 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1099s 154us/step - loss: 0.1662\n",
      "Iteration 4 [1110.7 s]: HR = 0.9223, loss = 0.1662 [18.7 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1112s 156us/step - loss: 0.1632\n",
      "Iteration 5 [1124.3 s]: HR = 0.9224, loss = 0.1632 [19.8 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1115s 157us/step - loss: 0.1601\n",
      "Iteration 6 [1128.0 s]: HR = 0.9237, loss = 0.1601 [18.9 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1131s 159us/step - loss: 0.1576\n",
      "Iteration 7 [1145.0 s]: HR = 0.9233, loss = 0.1576 [17.5 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1110s 156us/step - loss: 0.1552\n",
      "Iteration 8 [1122.3 s]: HR = 0.9227, loss = 0.1552 [18.2 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1096s 154us/step - loss: 0.1530\n",
      "Iteration 9 [1108.2 s]: HR = 0.9231, loss = 0.1530 [19.5 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1169s 164us/step - loss: 0.1511\n",
      "Iteration 10 [1182.7 s]: HR = 0.9239, loss = 0.1511 [18.6 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1121s 158us/step - loss: 0.1492\n",
      "Iteration 11 [1133.8 s]: HR = 0.9262, loss = 0.1492 [19.4 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1102s 155us/step - loss: 0.1475\n",
      "Iteration 12 [1113.8 s]: HR = 0.9210, loss = 0.1475 [18.4 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1067s 150us/step - loss: 0.1460\n",
      "Iteration 13 [1078.6 s]: HR = 0.9240, loss = 0.1460 [20.1 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1073s 151us/step - loss: 0.1445\n",
      "Iteration 14 [1084.6 s]: HR = 0.9258, loss = 0.1445 [18.0 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1123s 158us/step - loss: 0.1431\n",
      "Iteration 15 [1134.5 s]: HR = 0.9263, loss = 0.1431 [17.9 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1092s 153us/step - loss: 0.1418\n",
      "Iteration 16 [1103.8 s]: HR = 0.9190, loss = 0.1418 [17.5 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1119s 157us/step - loss: 0.1407\n",
      "Iteration 17 [1131.3 s]: HR = 0.9255, loss = 0.1407 [17.8 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 22358s 3ms/step - loss: 0.1398\n",
      "Iteration 18 [22369.6 s]: HR = 0.9243, loss = 0.1398 [16.9 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 986s 139us/step - loss: 0.1388\n",
      "Iteration 19 [996.0 s]: HR = 0.9257, loss = 0.1388 [17.2 s]\n",
      "End. Best Iteration 15:  HR = 0.9263. \n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 256\n",
    "\n",
    "model_out_file = '../pretrain/reddit_NeuMF_%d_%s_%d.h5' %(gmf_latent_dim, layers, time())\n",
    "\n",
    "t1 = time()\n",
    "hits = evaluate_model(model, test_positive, test_negative, topK)\n",
    "hr = np.array(hits).mean()\n",
    "print('Init: HR = %.4f\\t [%.1f s]' % (hr, time()-t1))\n",
    "\n",
    "best_hr, best_iter = hr, -1\n",
    "for epoch in range(epochs):\n",
    "    t1=time()\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels), batch_size=batch_size, \\\n",
    "                     epochs=1, shuffle=True)\n",
    "    \n",
    "    t2 = time()\n",
    "    hits = evaluate_model(model, test_positive, test_negative, topK)\n",
    "    hr, loss = np.array(hits).mean(), hist.history['loss'][0]\n",
    "    print('Iteration %d [%.1f s]: HR = %.4f, loss = %.4f [%.1f s]' \n",
    "        % (epoch,  t2-t1, hr, loss, time()-t2))\n",
    "    if hr > best_hr:\n",
    "        best_hr, best_iter = hr, epoch\n",
    "        model.save_weights(model_out_file, overwrite=True)\n",
    "        \n",
    "print(\"End. Best Iteration %d:  HR = %.4f. \" %(best_iter, best_hr))\n",
    "\n",
    "model_out_file = '../pretrain/reddit_NeuMF_%d_%s_%d.h5' %(gmf_latent_dim, layers, time())\n",
    "model.save_weights(model_out_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hit ratio was given at iteration 15. The model achieved a hit ratio of 0.9263. This means that 92% of the time, the model was able to correctly recommend a subreddit to a user.\n",
    "\n",
    "Finally, since we're mainly interested in the subreddit embeddings in order to calculate their similairty, we save those. We concatenate the 8-dimensional embedding from the GMF component and the 32-dimensional embedding from the MLP component to obtain a final 40-dimensional embedding vector. We normalize the vector and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../pretrain/reddit_NeuMF_8_[64, 32, 16, 8]_1563000005.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_item_embedding = model.get_layer('GMF_item_embedding')\n",
    "MLP_item_embedding = model.get_layer('MLP_item_embedding')\n",
    "\n",
    "GMF_item_weights = GMF_item_embedding.get_weights()[0]\n",
    "MLP_item_weights = MLP_item_embedding.get_weights()[0]\n",
    "\n",
    "NeuMF_item_weights = np.concatenate((GMF_item_weights, MLP_item_weights),axis=1)\n",
    "NeuMF_item_weights_norma = NeuMF_item_weights/np.linalg.norm(NeuMF_item_weights, axis = 1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('../data/neumf_weights.h5', 'w')\n",
    "h5f.create_dataset('neumf_weights', data=NeuMF_item_weights_norma)\n",
    "h5f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
