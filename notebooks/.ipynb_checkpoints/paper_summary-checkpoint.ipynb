{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Subreddit Recommender System Using Neural Collaborative Filtering\n",
    "\n",
    "Sources: https://arxiv.org/pdf/1708.05031.pdf, https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "\n",
    "Dataset: https://www.kaggle.com/colemaclean/subreddit-interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import keras\n",
    "import multiprocessing\n",
    "import math\n",
    "import heapq\n",
    "import json\n",
    "from time import time\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Embedding, Input, Dense, Multiply, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Collaborative filtering is a type of personalized recommender system that models user's preference on items based on their past interactions. Matrix factorization (MF) is the most popular method for collaborative filtering. The MF models user and item interaction as an inner product of their latent vectors. This paper focuses on using neural networks to learn the interaction function, rather than handcrafting it.\n",
    "\n",
    "It specifically focuses on the implicit feedback case, in which user interactions are reflected through behaviors like watching videos and purchasing products. In contrast, explicit feedback reflects user interaction through reviews and ratings of items. The implicit case is more difficult because user satisfaction is not observed and negative feedback is absent.\n",
    "\n",
    "## 2 Preliminaries\n",
    "\n",
    "### 2.1 Learning from Implicit Data\n",
    "\n",
    "Let M denote the number of users and N denote the number of items. The user-item iteraction matrix $\\textbf{Y} \\in \\mathbb{R}^{MxN}$ from user's implicit feedback is defined as:\n",
    "\n",
    "$$y_{ui}=\\begin{cases} \n",
    "          1 & \\text{if interaction (user u, item i) is observed} \\\\\n",
    "          0 & \\text{otherwise.} \n",
    "       \\end{cases}$$\n",
    "\n",
    "The problem is then to estimate the scores of all unobserved entries in $\\textbf{Y}$, which are used to rank the items, through an interaction function. The interaction function can be formalized as $\\hat{y}_{ui} = f(u,i|\\Theta)$, where $\\hat{y}_{ui}$ denotes the predicted score of interaction $y_{ui}$, $\\Theta$ denotes model parameters, and f denotes the function that maps model parameters to the predicted score.\n",
    "\n",
    "The parameters $\\Theta$ can be estimated by optimizing an objective function. Two commonly used functions are the pointwise loss and the pairwise loss. The pointwise approach looks at a single item at a time in the loss function. The pointwise approach looks at a pair of items at a time. Pairwise learning maximizes the margin between observed entry $\\hat{y}_{ui}$ and unobserved entry $\\hat{y}_{uj}$\n",
    "\n",
    "The presented NCF framwork parametrizes the function f using neural networks to estimate $\\hat{y}_{ui}$. It naturally supports both pointwise and pairwise learning.\n",
    "\n",
    "### 2.2 Matrix Factorization\n",
    "\n",
    "MF associates each user and item with a real-valued vector of latent features. Let $\\textbf{p}_u$ and $\\textbf{q}_u$ denote the latent vector for user u and item i, respectively. MF estimates an interaction $y_{ui}$ as the inner product of  $\\textbf{p}_u$ and $\\textbf{q}_u$:\n",
    "\n",
    "$$\\hat{y}_{ui}=f(u,i| \\textbf{p}_u,\\textbf{q}_u)=\\textbf{p}_u^T\\textbf{q}_u=\\sum_{k=1}^{K}p_{uk}q_{ik}$$\n",
    "\n",
    "where K denotes the dimension of the latent space. MF, however, has its limitations. Consider the following figure.\n",
    "\n",
    "<img src=\"../media/figure1.png\">\n",
    "\n",
    "Let's say we use the Jaccard coefficient to calculate the similarity between two users. Initially, we have $s_{23}>s_{12}>s_{13}$. Now, if we consider a new user $u_4$, where $s_{41}>s_{43}>s_{42}$, the MF model will place $\\textbf{p}_4$ closer to $\\textbf{p}_2$ than $\\textbf{p}_3$, which leads to a large ranking loss. A simple solution would be to use a large number of latent factors K to increase the latent space, but this may cause the model to overfit the data. The paper looks to address this limitation by learning the interaction function using DNNs from data.\n",
    "\n",
    "## 3. Neural Collaborative Filtering\n",
    "\n",
    "### 3.1 General Framework\n",
    "\n",
    "Here, we frame the collaborative filtering process as a multi-layer perceptron, where we input two feature vectors, $\\textbf{v}_u$ and $\\textbf{v}_i$, that describe user u and item i. \n",
    "\n",
    "<img src=\"../media/figure2.png\">\n",
    "\n",
    "Since, we're focused on the pure collaborative filtering case, the feature vectors are just the one-hot encodings of the user and the item. Then, the sparse user vector is passed through a fully connected layer that outputs the dense latent vector. The same is done for the item vector. \n",
    "\n",
    "The user and item latent vectors are then fed to a multi-layer neural architecture (neural collaborative filtering layers) that maps them to a prediction score.  The network is then trained through pointwise or pairwise learning. The paper focuses only on pointwise training.\n",
    "\n",
    "The predictive model can be formulated as such:\n",
    "\n",
    "$$\\hat{y}_{ui}=f(\\textbf{P}^T\\textbf{v}_u,\\textbf{Q}^T\\textbf{v}_i|\\textbf{P},\\textbf{Q},\\Theta)$$\n",
    "\n",
    "$$f(\\textbf{P}^T\\textbf{v}_u,\\textbf{Q}^T\\textbf{v}_i)=\\phi_{out}(\\phi_{X}(...\\phi_2(\\phi_1(\\textbf{P}^T\\textbf{v}_u,\\textbf{Q}^T\\textbf{v}_i))))$$\n",
    "\n",
    "Here, $\\phi_{out}$ denotes the mapping function for the output layer and $\\phi_X$ denotes the x-th neural collaborative filtering layer, and there are X neural CF layers in total.\n",
    "\n",
    "#### 3.1.1 Learning NCF\n",
    "\n",
    "We cab vuew tge vakye if $y_{ui}$ as a label, where 1 means item i is relevant to user u, and 0 otherwise. Therefore, we need to constrain the output $\\hat{y}_{ui}$ in the range of [0, 1] which can be done using the Logistic function as the activation function for the output layer $\\phi_{out}$.\n",
    "\n",
    "With the above settings, we define the loss function as \n",
    "\n",
    "$$L=-\\sum_{(u,i)\\in\\gamma\\cup\\gamma^-}y_{ui}log\\hat{y}_{ui}+(1-y_{ui})log(1-\\hat{y}_{ui})$$\n",
    "\n",
    "where $\\gamma$ denotes the set of observed interaction in $\\textbf{Y}$, and $\\gamma^-$ denotes the set of negative instances. This is the loss function to minimize and its omptimization can be done with stochastic gradient descent. For the negative instances $\\gamma^-$, we uniformly sample from unobserved instances in each iteration and control the sampling ratio w.r.t. the number of observed interactions.\n",
    "\n",
    "### 3.2 Generalized Matrix Factorization (GMF)\n",
    "\n",
    "Next, we show how MF is just a special case of the NCF architecture. We can think of the output of the embedding layer as the latent vectors for user and item. Let user latent vector $\\textbf{p}_u$ be $\\textbf{P}^T\\textbf{v}_u$ and item latent vector $\\textbf{q}_i$ be $\\textbf{Q}^T\\textbf{v}_i$. We define the mapping function of first neural CF layer as:\n",
    "\n",
    "$$\\phi_1(\\textbf{p}_u,\\textbf{q}_i)=\\textbf{p}_u\\odot\\textbf{q}_i)$$\n",
    "\n",
    "where $\\odot$ denotes element-wise product of vectors. Then, we project vector to the output layer:\n",
    "\n",
    "$$\\hat{y}_{ui}=a_{out}(\\textbf{h}^T(\\textbf{p}_u\\odot\\textbf{q}_i))$$\n",
    "\n",
    "If $a_{out}$ is an identity function and $\\textbf{h}$ is just a vector of 1's, the MF model can be recovered.\n",
    "\n",
    "The paper proposes a Generalized Matrix Factorization that uses the sigmoid function as $a_{out}$ and learns $\\textbf{h}$ from data with the log loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMF(X_user, X_item, num_users, num_items, latent_dim):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implementation of the Generalized Matrix Factorization\n",
    "    \n",
    "    Arguments:\n",
    "    X_user -- input tensor of shape (1,), specifying the user id\n",
    "    X_item -- input tensor of shape (1,), specifying the item id\n",
    "    num_users -- integer, number of total users \n",
    "    num_items -- integer, number of total items\n",
    "    latent_dim -- dimension of the embedding latent vector\n",
    "    initializer -- initializer for the embeddings matrix\n",
    "    regs -- float list of size 2, specifying the regularization parameters for both embedding layers\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the GMF, tensor of shape (1,), specifies the likelihood that X_item is relevant to X_user\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer = initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=1)\n",
    "    \n",
    "    X_user = Embedding(input_dim=num_users, output_dim=latent_dim, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='GMF_user_embedding')(X_user)\n",
    "    X_item = Embedding(input_dim=num_items, output_dim=latent_dim, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='GMF_item_embedding')(X_item)\n",
    "    \n",
    "    X_user = Flatten()(X_user)\n",
    "    X_item = Flatten()(X_item)\n",
    "    \n",
    "    X = Multiply()([X_user, X_item])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_GMF_model(num_users, num_items, latent_dim, initializer='uniform'):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = GMF(X_user, X_item, num_users, num_items, latent_dim, initializer)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Layer Perceptron (MLP)\n",
    "\n",
    "The GMF only uses a fixed element-wise product between the two latent vectors to model their interactions. More flexibility and non-linearity can be obtained by also concatenating the two latent vectors and feeding the concatenation to a standard Multi-Layer Perceptron that can learn the interaction between the user and the item.\n",
    "\n",
    "The MLP model is defined as:\n",
    "\n",
    "$$\\textbf{z}_1=\\phi_1(\\textbf{p}_u,\\textbf{q}_i)=[\\textbf{p}_u\\;\\textbf{q}_i]^T$$\n",
    "\n",
    "$$\\phi_2(\\textbf{z}_1)=a_2(\\textbf(W)_2^T\\textbf{z}_1+\\textbf{b}_2)$$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$\\phi_L(\\textbf{z}_{L-1})=a_L(\\textbf{W}_L^T\\textbf{z}_{L-1}+\\textbf{b}_L)$$\n",
    "\n",
    "$$\\hat{y}_{ui}=\\sigma(\\textbf{h}^T\\phi_L(\\textbf{z}_{L-1}))$$\n",
    "\n",
    "where $\\textbf{W}_x$, $\\textbf{b}_x$, and $a_x$ denote the weight matrix, bias vector, and activation function for the x-th layer's perceptron, respectively. Various activation functions can be chosen for the MLP layers, but the paper opts to use the ReLU, because it is more biologically plausible, proven to be non-saturated, and encourages sparse activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X_user, X_item, num_users, num_items, layers = [20, 10]):\n",
    "\n",
    "    \"\"\"\n",
    "    Implementation of the Multi-Layer Perceptron\n",
    "    \n",
    "    Arguments:\n",
    "    X_user -- input tensor of shape (1,), specifying the user id\n",
    "    X_item -- input tensor of shape (1,), specifying the item id\n",
    "    num_users -- integer, number of total users \n",
    "    num_items -- integer, number of total items\n",
    "    initializer -- initializer for the embeddings matrix\n",
    "    layers -- integer list, specifying the units for each layer\n",
    "    regs -- float list, specifying the regularization parameters for each layer, reg_layers[0] is for the embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the MLP, tensor of shape (1,), specifies the likelihood that X_item is relevant to X_user\n",
    "    \"\"\"\n",
    "    \n",
    "    initializer = initializers.RandomUniform(minval=-0.01, maxval=0.01, seed=1)\n",
    "    \n",
    "    X_user = Embedding(input_dim=num_users, output_dim=layers[0]//2, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='MLP_user_embedding')(X_user)\n",
    "    X_item = Embedding(input_dim=num_items, output_dim=layers[0]//2, embeddings_initializer=initializer, \\\n",
    "                                 input_length=1, name='MLP_item_embedding')(X_item)\n",
    "    \n",
    "    X_user = Flatten()(X_user)\n",
    "    X_item = Flatten()(X_item)\n",
    "    \n",
    "    X = Concatenate()([X_user, X_item])\n",
    "\n",
    "    for i in range(1, len(layers)):\n",
    "        X = Dense(layers[i], activation='relu', name='layer'+str(i))(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_MLP_model(num_users, num_items, layers=[20,10]):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = MLP(X_user, X_item, num_users, num_items, initializer, layers)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Fusion of GMF and MLP\n",
    "\n",
    "The one-hot encoding user and item vectors can be fed to two different embeddings, one for the GMF and one for the MLP. Then, the two models can be combined by concatenating their last hidden layer. The fused model can be pictured below.\n",
    "\n",
    "<img src=\"../media/figure3.png\">\n",
    "\n",
    "The final fused model can be formulated as such:\n",
    "\n",
    "$$\\phi^{GMF}=\\textbf{p}_u^G\\odot\\textbf{q}_i^G$$\n",
    "\n",
    "$$\\phi^{MLP}=a_L(\\textbf{W}_L^T(a_{L-1}(...a_2(\\textbf{W}_2^T[\\textbf{p}_u^M\\;\\textbf{q}_i^M]^T+\\textbf{b}_2)...))+\\textbf{b}_L)$$\n",
    "\n",
    "$$\\hat{y}_{ui}=\\sigma(\\textbf{h}^T[\\phi^{GMF}\\;\\phi^{MLP}]^T)$$\n",
    "\n",
    "where $\\textbf{p}_u^G$ and $\\textbf{p}_u^M$ denote the user embedding for GMF and MLP parts, and similar notations of $\\textbf{q}_i^G$ and $\\textbf{q}_i^M$ for item embeddings. This model is dubbed as \"NeuMF\", short for Neural Matrix Factorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuMF(X_user, X_item, num_users, num_items, gmf_latent_dim=10, layers=[20, 10]):\n",
    "    X_GMF = GMF(X_user, X_item, num_users, num_items, gmf_latent_dim)\n",
    "    X_MLP = MLP(X_user, X_item, num_users, num_items, layers)\n",
    "    \n",
    "    X = Concatenate()([X_GMF, X_MLP])\n",
    "    \n",
    "    return X\n",
    "\n",
    "def get_NeuMF_model(num_users, num_items, gmf_latent_dim=10, layers=[20, 10]):\n",
    "    X_user = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    X_item = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    X = NeuMF(X_user, X_item, num_users, num_items, gmf_latent_dim, layers)\n",
    "    \n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(X)\n",
    "    \n",
    "    model = Model(inputs=[X_user, X_item], outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Pre-training\n",
    "\n",
    "For training, one seeks to minimize the object function of NeuMF. However, due to the function's non-convexity, one can only find local solutions using gradient-based optimization. Initialization plays an important role for the convergence and performance of deep learning models. The paper proposes to first train GMF and MLP, first. Then, their model parameters are used as the initialization for the corresponding parts of NeuMF's parameters. In the output layer, the weights of the 2 models are concatenated:\n",
    "\n",
    "$$\\textbf{h}=[\\alpha\\textbf{h}^{GMF}\\;(1-\\alpha)\\textbf{h}^{MLP}]^T$$\n",
    "\n",
    "where $\\textbf{h}^{GMF}$ and $\\textbf{h}^{MLP}$ denote the $\\textbf{h}$ vector of the pretrained GMF and MLP model, respectively, and $\\alpha$ is a hyper-parameter determining the trade-off between the two pre-trained models.\n",
    "\n",
    "For training GMF and MLP from scratch, Adaptive Moment Estimation (Adam) is used. After feeding the pre-trained parameters into NeuMF, the ensemble model is optimized with Vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "num_negatives = 4\n",
    "batch_size = 256\n",
    "layers = [64,32,16,8]\n",
    "topK = 10\n",
    "gmf_latent_dim=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/reddit_train_10.csv', header=None)\n",
    "df_test_positive = pd.read_csv('../data/reddit_test_positive_10.csv', header=None, usecols=[0,1])\n",
    "df_test_negative = pd.read_csv('../data/reddit_test_negative_10.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = max(df_train.iloc[:, 0])\n",
    "num_items = max(df_train.iloc[:, 1])\n",
    "\n",
    "train = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "for i, row in df_train.iterrows():\n",
    "    user = row[0]\n",
    "    item = row[1]\n",
    "    train[user, item] = 1.0\n",
    "    \n",
    "test_positive = [(row[0], row[1]) for _, row in df_test_positive.iterrows()]\n",
    "test_negative = [row[1:100].values.flatten().tolist() for _, row in df_test_negative.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_NeuMF_model(num_users+1, num_items+1, layers=layers, gmf_latent_dim=gmf_latent_dim)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building Dataset\n",
    "\n",
    "### Evaluation Protocols\n",
    "\n",
    "The paper uses leave-one-out method to evaluate the performance of item recommendation algorithms. Basically, for each user, the last interaction is held-out. All held-out interactions are used as the test set, and the rest are used for training. Then, for each user, 100 items are randomly sampled, and then ranked. These are items that the user hasn't interacted with before. This method separates our dataset into three files: train.rating, test.rating, test.negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, pos_item):\n",
    "    for item in ranklist:\n",
    "        if item == pos_item:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_model(model, test_positive, test_negative, K):\n",
    "    hits = []\n",
    "    for i in range(len(test_positive)):\n",
    "        \n",
    "        rating = test_positive[i]\n",
    "        items = test_negative[i]\n",
    "        user, pos_item = rating\n",
    "        items.append(pos_item)\n",
    "        \n",
    "        map_item_score = {}\n",
    "        \n",
    "        users = np.full(len(items), user, dtype='int32')\n",
    "        predictions = model.predict([users, np.array(items)], batch_size=100, verbose=0)\n",
    "        for i in range(len(items)):\n",
    "            map_item_score[items[i]] = predictions[i]\n",
    "        items.pop()\n",
    "        \n",
    "        ranklist = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "        hr = getHitRatio(ranklist, pos_item)\n",
    "        hits.append(hr)\n",
    "        \n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: HR = 0.0950\t [20.3 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1281s 180us/step - loss: 0.2136\n",
      "Iteration 0 [1294.3 s]: HR = 0.8944, loss = 0.2136 [18.7 s]\n",
      "Epoch 1/1\n",
      "7113985/7113985 [==============================] - 1051s 148us/step - loss: 0.1897\n",
      "Iteration 1 [1062.9 s]: HR = 0.9055, loss = 0.1897 [19.3 s]\n",
      "Epoch 1/1\n",
      "1945088/7113985 [=======>......................] - ETA: 12:25 - loss: 0.1775"
     ]
    }
   ],
   "source": [
    "model_out_file = '../pretrain/reddit_NeuMF_%d_%s_%d.h5' %(gmf_latent_dim, layers, time())\n",
    "\n",
    "t1 = time()\n",
    "hits = evaluate_model(model, test_positive, test_negative, topK)\n",
    "hr = np.array(hits).mean()\n",
    "print('Init: HR = %.4f\\t [%.1f s]' % (hr, time()-t1))\n",
    "\n",
    "best_hr, best_iter = hr, -1\n",
    "for epoch in range(epochs):\n",
    "    t1=time()\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels), batch_size=batch_size, \\\n",
    "                     epochs=1, shuffle=True)\n",
    "    \n",
    "    t2 = time()\n",
    "    hits = evaluate_model(model, test_positive, test_negative, topK)\n",
    "    hr, loss = np.array(hits).mean(), hist.history['loss'][0]\n",
    "    print('Iteration %d [%.1f s]: HR = %.4f, loss = %.4f [%.1f s]' \n",
    "        % (epoch,  t2-t1, hr, loss, time()-t2))\n",
    "    if hr > best_hr:\n",
    "        best_hr, best_iter = hr, epoch\n",
    "        model.save_weights(model_out_file, overwrite=True)\n",
    "print(\"End. Best Iteration %d:  HR = %.4f. \" %(best_iter, best_hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_file = '../pretrain/reddit_NeuMF_%d_%s_%d.h5' %(gmf_latent_dim, layers, time())\n",
    "model.save_weights(model_out_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = get_NeuMF_model(num_users+1, num_items+1, layers=layers, gmf_latent_dim=gmf_latent_dim)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "best_model.load_weights('../pretrain/reddit_NeuMF_8_[64, 32, 16, 8]_1562889457.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_item_embedding = model.get_layer('GMF_item_embedding')\n",
    "MLP_item_embedding = model.get_layer('MLP_item_embedding')\n",
    "\n",
    "GMF_item_weights = GMF_item_embedding.get_weights()[0]\n",
    "MLP_item_weights = MLP_item_embedding.get_weights()[0]\n",
    "\n",
    "NeuMF_item_weights = np.concatenate((GMF_item_weights, MLP_item_weights),axis=1)\n",
    "\n",
    "MLP_item_weights_norma = MLP_item_weights/np.linalg.norm(MLP_item_weights, axis = 1).reshape((-1, 1))\n",
    "GMF_item_weights_norma = GMF_item_weights/np.linalg.norm(GMF_item_weights, axis = 1).reshape((-1, 1))\n",
    "NeuMF_item_weights_norma = NeuMF_item_weights/np.linalg.norm(NeuMF_item_weights, axis = 1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/subreddit.json') as f:\n",
    "    [d, inv_d] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit: magictcg               Similarity: 1.0\n",
      "Subreddit: nomansskythegame       Similarity: 0.899\n",
      "Subreddit: magicarena             Similarity: 0.874\n",
      "Subreddit: gamernews              Similarity: 0.86\n",
      "Subreddit: witcher                Similarity: 0.86\n",
      "Subreddit: masseffect             Similarity: 0.857\n",
      "Subreddit: starcitizen            Similarity: 0.856\n",
      "Subreddit: fantheories            Similarity: 0.845\n",
      "Subreddit: starwarsbattlefront    Similarity: 0.842\n",
      "Subreddit: heroesofthestorm       Similarity: 0.842\n",
      "Subreddit: pathofexile            Similarity: 0.84\n",
      "Subreddit: fo4                    Similarity: 0.838\n",
      "Subreddit: dndnext                Similarity: 0.836\n",
      "Subreddit: competitiveoverwatch   Similarity: 0.825\n",
      "Subreddit: defenders              Similarity: 0.825\n",
      "Subreddit: hearthstone            Similarity: 0.824\n",
      "Subreddit: thedivision            Similarity: 0.822\n",
      "Subreddit: rpg                    Similarity: 0.821\n",
      "Subreddit: totalwar               Similarity: 0.816\n",
      "Subreddit: darksouls              Similarity: 0.813\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(subreddit, num_recommendations):\n",
    "    index = inv_d[subreddit.lower()]\n",
    "    dists = np.dot(NeuMF_item_weights_norma, NeuMF_item_weights_norma[int(index)])\n",
    "    sorted_dists = np.argsort(dists)\n",
    "    closest = sorted_dists[-num_recommendations:]\n",
    "    max_width = max([len(d[str(c)]) for c in closest])\n",
    "    for c in reversed(closest):\n",
    "        print(f'Subreddit: {d[str(c)]:{max_width + 2}} Similarity: {dists[c]:.{3}}')\n",
    "        \n",
    "get_recommendations('magictcg', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
